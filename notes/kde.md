### Using kernel density estimation to search inside of long texts

#### Documents vs. Texts

Back when I was working at Virginia, Wayne Graham and I were tasked with the job of moving a big collection of old text encoding projects that had been developed at the Etext Center, a precursor to the Scholars' Lab, onto a more up-to-date server architecture. The Etext Center was one of the earliest and most venerable digital humanities outfits at Virgnia - and, in many ways, an early blueprint for the modern digital humanities center. For 15 years, from 1992 through 2007, a merry troupe of students, professors, librarians, and staff pumped out thousands of digital editions of literary and historical texts. At core, it was a TEI encoding shop - most of the projects took the form of high-fidelity, interpretive, TEI-backed editions of individual texts, or smallish groups of texts - the collected works of Mark Tain, say, as opposed to projects that work at the scale of tens or hundreds of thousands of texts.

For years, all of these projects were hosted on a single physical server in the library, which, by the time I showed up in 2011, was ancient and out of warranty, making it a ticking time bomb from an operations standpoint - everything had to get moved over to the library's modern server infrastructure. For the most part, this was pretty straightforward from a technological standpoint. Most of the projects consisted of a set of TEI documents - usually the core work product - and a set of XSLT stylesheets that transformed the XML into HTML for presentation on the web. For each project, we just grepped through all the XML files and scrubbed out hard-coded links to the old Etext website, plugged the scripts that generated the HTML into the Capistrano-backed deployment rig that powered the production servers, and then just shovelled all the files onto the new servers.

Again and again, though, there was one big sticking point - many of the projects had some kind of full-text searching functionality, which is a much harder to migrate than nice, static XML/XSLT files. To make things worse, most of the projects used an ancient and long-defunct XML search utility called XXX, meaning that we had to more or less rewrite everything from scratch. Fortunately, open-source search technologies are much better now than there were in the mid-90's, and we took a pretty standard approach to the problem - write some scripts to split up the TEI documents into little chunks (usually just breaking on some paragraph- or section-level unit of markup, like `<div2>` or `<p>`, depending on how the TEI was structured), and index all of the sub-documents in Solr, which fed results into a barebones little web interface that exposed all of the goodies that come with Solr - hit-highlighting, faceting, pagination, etc.

This is an easy and effective solution to the problem, and Solr's hit-highlighting did a good job of replicating the functionality of the old software, which was basically generated a stack of keyword-in-context concordance results. But something about this always kind of chafed at me, from some kind of philosophical standpoint. I couldn't put a finger on why, at first, but after rolling it around in my head for a while I realized that the root of the problem was actually pretty low-level and fundamental, something far too open-ended to be tackled in the context of the server migrations. Really, we were bumping up against the limitations of a basic unit of information that software systems like Solr are built around - the idea of the "document."

In the context of information retrieval, a document is basically a little packet of content that might contain some kind of information that you care about, usually mixed in with lots of other similarly-structured packets - a "corpus," "database," or "document set." This idea is so ubiquitous - and, in most cases, so obviously useful - that it barely needs to be rehearsed. Almost every digital object that we interact with on a computer is modeled as a document at one level or another. Web pages are documents. This blog post is a document. Tweets, product listings, user profiles, Omeka items, Neatline records - all documents. A key feature of documents, defined in this way, is that they are thought of as being discrete and self-contained, for the most part - when you have some kind of question you want to ask of the document collection, the assumption is that the answer lies inside exactly of one of the documents (or, at least, exactly one document contains the _best_ answer to the question). The task of searching, then, becomes conceptually simple - given some query, find a subset of the documents that are relevant, and then order the documents by descending relevance, so that the closest match sits at the top of the stack.

This makes sense, because a huge amount of information takes the form of a big collection of documents. If I go to the digital catalog search interface in a library - Virgo at UVa, SearchWorks at Stanford - and type in "War and Peace," I want to find, well, _War and Peace_ - one particular document in the library. In the simplest case, it's basically a hash lookup. More likely, though, you probably don't know exactly which document you want, but instead know the _kind_ of document that you want. For example, if I'm working on a piece of software and run into some kind of weird error message, I usually just paste the entire error output into Google. I don't know which exact page I want - it could be a StackOverflow answer, or a thread on a Google group, a blog post - but do know that I just want one page that tells me exactly what's wrong an exactly how to fix it - the _best_ answer to the question, if not the only one. Search is a filtering process, a reduction from many to one - the right file in the cabinet, the right card in the catalog.

But this analogy breaks down in some really interesting and dramatic ways when applied to literary texts - and the types of question that are interesting to ask about them.







In information retrieval, a document is a discrete little packet of information - a cluster, a nodule, a data point that can be manipulated and analyzed as a single, unified entity. Documents aren't considered to be completely homogenous - in topic modelling, for example, documents are understood to have the ability to exhibit multiple topics, for example, to be about more than one thing. But, fundamentally, a document is the smallest unit of analysis

But this runs completely counter to the experience of a literary text, which fundemtanlly exists an an a progression, a narrative, a textual _interval_ that begins at the first word of the text and extends - both in a typographical and structural sense, and in a temporal sense, for any individual reading experience - to the last word of the text. This experience of a text as a space or an interval, a one-dimensional axis that is

If a document is a point, then a literary text is a line.


## Segment length whack-a-mole


it's almost more of a spatial question -
