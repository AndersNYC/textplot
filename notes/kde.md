### Using kernel density estimation to "photograph" texts

Earlier in the summer, I was thinking about the way that words distribute inside of long texts - the way they slosh around, ebb and flow, clump together in some parts but not others. Some words don't really do this at all - they're spaced evenly throughout the document, and their distribution doesn't say much about the overall structure of the text. This is certainly true for stopwords like "the" or "an," but it's also true for lots of words that carry more semantic information but aren't really associated with any particular content matter. For example, think of words like "quickly" or "never" - they're generic terms, free-agents that could be used in almost any context.

Other words, though, are much more semantically focused - they occur unevenly, and they tend to hang together with other words that orbit around the same content matter. For example, think of a long novel like _War and Peace_, which contains dozens of different plot lines, characters, scenes, themes, motifs, etc. There are battles, dances, hunts, meals, duels, salons, parlors, and so on and so forth - and, in the broadest sense, the "war" sections and the "peace" sections. Some words are really closely associated with one of these topics, and can be used as proxies for how it distributes in the text. If you open to a random page and see words like "Natasha," "dancing," "family," "marry," or "children," it's a pretty good bet that you're in a peace-y section. But if you see words like "Napoleon," "Borodino," "horse," "fire," "cannon," or "guns," it's probably a war section. Or, at a more granular level, if you see words like "historian" or "clock" or "inevitable," it's probably one of those pesky historiographic essays.

To borrow Franco Moretti's term, I was tyring to think of a way to "operationalize" these distributions - a standardized way to generate some kind of profile that would capture the structure of the locations of a term inside a document, ideally in a way that it would be possible to compare it with with the distributions of other words. I started poking around, and quickly discovered that if you know anything about statistics (I really don't, so take all of this with a grain of salt), there's a really simple and obvious way to do this - a kernel density estimate, which takes an observed set of data points and works backward to approximate a probabilty density function that, if you sampled it the same number of times, would produce more or less the same set of data.

Kernel density estimation (KDE) is really simple - unlike the math behind something like topic modeling, which gets complicated pretty fast, KDE is basically just addition. Think of the text as a big X-axis, where each integer corresponds to a word position in the text. So, for _War and Peace_, the text would stretch from the origin to the X-axis offset of 573,064, the number of words in the text. Then, any word in the text can be plotted just by laying down ticks on the X-axis at all the offsets where the word shows up in the text. For example, here's "horse" in _War and Peace_:

[fig]

This can be converted into a histogram by chopping up the X-axis into a set of evenly-spaced bins and drawing a bar up to a value on the Y-axis that representis the number of data points that fall inside that segement:

[fig]

A kernel density estimate is the same idea, except, instead of just counting points, each of the points is represented as a "kernel" function centered around that point. A kernel is just some kind of weighting function that models a decay in intensity around the data point. At the very simplest, it could be something like the triangular kernel, which just draws a pair of straight, angled lines down to the X-axis, but most applications use something smooth and gradual, like the Epanechnikov or Gaussian functions. I'm sure there are intelligent reasons for why you might prefer one over another, but, for the purposes of this project, they all give basically the same results.

[fig]

The important thing, though, is that the kernel transforms the point into a two-dimensional area that captures an interval within which the point as some kind of abstract significance, and the distribution of that significance inside the interval. This is cool because it maps realy well onto basic intuitions about the "scope" of each individual word in a text. When you come across a word, _where_ exactly does it have significance? Definitely right there, at the exact location where it appears, not _just_ there - it also makes sense to think of a kind of "relevance" or "meaning energy" that dissipates around the word, slowly at first in the immediately surrounding words and then more quickly as the distance increases. Words aren't self-contained little atoms of information - they radiate meaning forward and backward onto one another, both at a grammatical level inside of a sentence but also in a more diffuse, phenomenological sense. As you slide your eyes across a text, each word carries a kind of pyschological energy that spikes highest at the moment that you actually encounter it. But that energy doesn't instantly materialize and dematerialize - the word casts a shadow of meaning onto the words that follow it, and, in the context of an actual reading experience, it casts a shadow backwards onto the words that come before it. The kernel, then, is a crude but useful way to formalize the "meaning-shape" of a word as it appears to the psychological gaze of the reader, the region illuminated as the spotlight of attention pans across the text.

Anyway, once the all of the kernels are in place, estimating the kernel density is just a matter of stepping through each position on the X-axis and adding up the values of all the kernel functions at that particular location. This gives a single, composite curve
