# Using kernel density estimation to search inside of long texts

## Literary texts and the segmentation problem

Back when I was working at Virginia, between 2011 and the first half of 2014, Wayne Graham and I were tasked with the (long, difficult) process of moving a large collection of old text encoding projects that had been developed under the auspices of the eText Center onto a more modern server architecture. The eText Center was one of the earliest, longest-running, and most venerable digital humanities outfits at Virgnia (or really anwhere for that matter). It was one of the real trailblazers during what might now be thought of as the "first wave" of digital humanities work in the 90's and aughts, when most of the work was still centered around the task of digitizing texts and finding efficient and useful ways to make them available on the internet. At core, it was a TEI encoding shop, and most of the projects took the form of TEI-backed editions of texts or small groups of texts, which were carefully edited and wrapped up in the form of interpretive websites that provided really comprehensive and rich views of the documents.

For 15 years, from 1992 through 2007, almost all of the projects produced by the center had been put on a single physical server in the library, which, by the time I showed up in 2011, was ancient and literally out of warranty, which made it a kind of ticking time bomb from a systems administration standpoint. All of the projects had to get moved onto the library's up-to-date, production servers, but, as it turns out, this is no small task. In fact, the migration process had already been underway in some for or another for almost three years before I even got there, and Wayne and I were really just wrapping up the last little odds and ends. For the most part, the projects weren't hugely complicated from a technological standpoint - in most cases, they consisted of a core set of TEI documents, which was the core work product, and a set of XSLT stylesheets to transform the XML into HTML for presentation on the web. For each projects, we grepped through all the XML files and replaced harcoded links to the old Etext website, plugged the TEI -> HTML generation scripts into the Capistrano-backed deployment rig that powered the modern infrastructure, and just shovelled all the files onto the new servers.

Again and again, though, there was one big sticking point - as you might expect for projects that, at core, are digital editions of primary texts designed with researchers in mind, many of the projects had some kind of full-text searching functionality, which is a much bigger headache to migrate than nice, static XML/XSLT files. To make things worse, many of the search interfaces were built on top of an ancient piece of software for searching inside of XML documents called XXX, which is now completely defunct, meaning that we had to rewrite the search utilities completely from scratch for every project. Fortunately, open-source search technologies are vastly better now than there were in the mid-90's, and we took a pretty kosher approach to the problem - write some little scripts to split up the TEI documents into little chunks (usually just breaking on some roughly paragraph- or section-level unit of markup, like `<div2>` or `<p>`, depending on how the TEI was structured, and index all of the little sub-documents in Solr, which could then be queried by a simple search interface that exposed all of the built-in goodies that come with Solr - hit-highlighting, faceting, pagination, etc.

This is an easy and effective solution to the problem, and Solr's hit-highlighting did a good job of replicating the functionality of the old software, which was basically generated a stack of keyword-in-context concordance results. But, from a kind of philosophical standpoint, something about this irked me. It seemed obtuse, in some vague way, or at least less-than-optimal. I couldn't really put a finger on why, at first, but after rolling it around in my head for a while I realized that the problem was actually kind of large and fundamental, something far too open-ended to be tackled in the context of the migrations. Really, we were chafing up against the limitations of what is arguably the basic unit of organization in information retrieval - the idea of the "document."

A document is basically a little atom of content, mixed in with lots of other similarly-strucutred atoms of content (the "corpus" or "document set") that might contain some kind of information that you care about. This idea is so ubiquitous - and, in most cases, so clearly useful - that barely needs to be rehearsed. Documents are everywhere, and underpin basically all modern information systems. Web pages are documents. This blog post is a document. Tweets, product listings, user profiles, Omeka items, Neatline records - all documents. A key feature of documents, defined in this way, is that they are thought of as being discrete and self-contained - when you have some kind of question, an information need, the assumption is that the answer lies inside exactly of one of the documents (or, at least, exactly one document contains the _best_ answer to the question). The task of searching, then, becomes conceptually simple - given some query, find a subset of the documents that are relevant, and then order the documents by descending relevance, so that the closest match sits at the top of the stack. The assumption is that there's one document that is the _best_ or _correct_ "answer" to the query, and that the job of the information system is to pick out that document. Information is a file cabinet, and information retrieval is just matter of finding the right file.

In most cases, this works really well. If I go to the digital catalog search interface in a library - Virgo at UVa, SearchWorks at Stanford - and type in "War and Peace," I want to find, well, _War and Peace_ - one, particular "document" in the library. In the simplest case, it's basically a hash lookup. More likely, you might not know exactly which document you want, but instead have a set of criteria that need to be matched. For example, if I hit a weird problem while programming and paste the error output into Google, I don't know which exact page I want - it could be a StackOverflow answer, or a thread on a Google group, a blog post - but do know that I just want the one, individual page that tells me exactly what's wrong an exactly how to fix it - the _best_ answer to the question, if not the only one. Or, if I go to Amazon, and search for "digital camera" - I don't know which camera I want, and in this case I'm probably most interested in looking at multiple "documents" and comparing them to one another. But again, in the end, the _answer_ will be one particular camera - the one that I decide to buy.

But this analogy breaks down when applied to literary texts, and the kinds of questions that are interesting to ask about them. Say you study Shakespeare. Where do the "documents" begin and end? In the simplest sense, you could say that every play is a document. So, the



Is each individual play a separate


In a simplistic and naive sense, you could say that each play is a separate document, fold the sonnets together (and maybe some of the other early poetry) into a document,



In information retrieval, a document is a discrete little packet of information - a cluster, a nodule, a data point that can be manipulated and analyzed as a single, unified entity. Documents aren't considered to be completely homogenous - in topic modelling, for example, documents are understood to have the ability to exhibit multiple topics, for example, to be about more than one thing. But, fundamentally, a document is the smallest unit of analysis

But this runs completely counter to the experience of a literary text, which fundemtanlly exists an an a progression, a narrative, a textual _interval_ that begins at the first word of the text and extends - both in a typographical and structural sense, and in a temporal sense, for any individual reading experience - to the last word of the text. This experience of a text as a space or an interval, a one-dimensional axis that is
